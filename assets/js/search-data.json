{
  
    
        "post0": {
            "title": "NBA Injury Report Sports Analytics Project (Part 1)",
            "content": "Introduction . How To Read This Post . Project Description . It all started with a tweet from basketball analytics legend and Korean cinema aficionado Ed Kupfer: If somebody wants a project, scrape the data from the NBA injury report PDFs from the last few seasons. Just change the date in the URL. https://t.co/SnKpFXPrRl . &mdash; Ed Kupfer (@EdKupfer) April 23, 2021 . Pretty straightforward, right? . Kickstarter Model&#8482;&#65039; Goals . As per The Kickstarter Model™️, we need to define our three goals. At first blush, we might suggest something like the following: . Fallback Goal: &quot;Get the PDFs&quot; | Baseline Goal: &quot;Extract data from the PDFs&quot; | Stretch Goal: &quot;Present the data somehow&quot; | . That&#39;s absolutely defensible, and has the bonus of structurally aligning with the major technical &quot;tasks&quot; of the project. But I&#39;m not sure a directory full of PDFs qualifies as a &quot;dataset&quot; you could pass on to someone else as a contribution. Instead, we&#39;ll go with a slightly different approach (but feel free to use the above if it works better for you). . 1. Fallback Goal . Download and &quot;lift&quot; the data from the NBA Injury Report PDFs into some sort of machine-readable format (CSV, JSON, etc.) . Since &quot;getting the data&quot; is kind of the main point of this entire exercise, even though extracting data from the PDFs requires requires some [at this point] unknown tooling, we&#39;re going to have both downloading and processing the PDFs be be our Fallback Goal. If we don&#39;t do anything else, or if nothing else &quot;works&quot;, at least we&#39;ll have accomplished something useful that we can share with the community. . 2. Baseline Goal . Present the data we obtained above in some sort of webapp/dashboard/etc. . Structuring things this way now aligns with what we normally consider a &quot;Baseline&quot; Goal:A fully-fledged &quot;deliverable&quot; of some sort that provides value to the community.Doing this may sound a bit aggressive in terms of complexity, but our &quot;webapp&quot; doesn&#39;t need to be all that fancy. It&#39;s just a &quot;skin&quot; to slice and dice the data we generate from the Fallback Goal. It should be reasonably straightforward10. . 3. Stretch Goal . None . As we were pretty aggressive with our first two goals, I think it&#39;s perfectly reasonable to not have a Stretch Goal. Normally, you might think to say something vague like &quot;analyze the injury data&quot;, but in my opinion, that&#39;s actually counterproductive. The reason being that the goal of this project isn&#39;t really analysis! The goal is is unlock some data in one format, and present it to &quot;the user&quot; in another format that&#39;s more amenable for their absorption. This may not sound very &quot;sexy&quot;, but the reality is that this data collection/&quot;chewing&quot;/presenting process is basically one of the most important things in sports analytics.So unless you have a specific analysis in mind, I think it makes sense to not have a Stretch Goal, and to just focus on the first two goals. If we do this right, perhaps something interesting will fall out of our dashboard, and we can explore it at that point. . . Note: Again, the point of setting these goals isn&#8217;t to spur endless debate or to create another roadblock to starting a project. It&#8217;s just a way of structuring potential future design decisions, and making sure things like feature creep and &quot;great-being-the-enemy-of-good&quot; don&#8217;t impede actually delivering on the goals. . Now that we have a conceptual goal framework in which to work, let&#39;s get to it! . Fallback Goal: Downloading and Processing NBA Injury Report PDFs . The Kickstarter Model™️ is there to help us structure our effort wisely, and make productive high level design decisions, but it doesn&#39;t really say anything about how to do whatever it is we want to do! To that end, once we have our goals, I find it&#39;s helpful to spend a little time — before coding! — structuring my thinking about each of the tasks at hand. . Divvying Up Tasks . As per the discussion above, for our Fallback Goal, it seems like we already have two &quot;high level&quot; tasks: . Downloading PDFs | Processing PDFs. | But we can break those down a little further. First off, we can&#39;t download anything without knowing what to download. Ed&#39;s tweet mentions &quot;just change the date&quot;, it seems safe to assume we&#39;ll need to build a list of URLs based on dates, unless there&#39;s some central page with links to all of the injury reports, in which case we can just scrape the URLs from that. . And, of course, at this point I&#39;m not sure exactly how to extract the data from the PDFs (minor detail, right?!?) But I&#39;m pretty confident it can be done, so we&#39;ll save a little &quot;Google-fu&quot; for later. However, it does seem reasonable to use Pandas to post-process our data once we do get it out of the PDF, since it&#39;s definitely going to be tabular. Lastly, we might want to fancy with asynchronous requests (or perhaps not! It might not be worth the effort, but let&#39;s note it here). . Taking all of the above into account, our Fallback Goal task outline might look like this: . Download PDFs Generate list of URLs (dates) to injury report PDFs | Download URLs from list (do this asynchronously?) | | Process PDFs Extract data from PDF (with what?) | Post-process data (Pandas) | | . Tip: One thing we&#8217;ll find as we progress is that this outline, as pedantic as it may seem, is incomplete. Digging into the above steps, we&#8217;ll invariably find places where the sub-steps aren&#8217;t necessarily or obviously entailed by the macro steps. If this gets too far out of hand — when, exactly, this happens is a bit of a judgment call — it makes sense to go back and revise the outline. For the sake of this blog post, I&#8217;m going to go forward with this level of granularity, and we&#8217;ll see how far we can get. . Now we can tackle these individually, and make sure we&#39;re always progressing toward the end goal! . Task 1: Downloading The Injury Reports . Before we can actually do anything interesting with the injury reports, we need to download them. To do that, we need to know where there are. After some brief googling, it seems like there really isn&#39;t a central repository for these reports, and they&#39;re just uploaded one at a time20. . The one example URL from Ed&#39;s tweet looks like this: . https://ak-static.cms.nba.com/referee/injury/Injury-Report_2021-04-23_01PM.pdf . Seems like there&#39;s 4 parts to the URL: . Some boilerplate: https://ak-static.cms.nba.com/referee/injury/Injury-Report_ | A date: 2021-04-23 | A time: 01PM | The file extension: .pdf | We&#39;re only interested in the middle two (the date and the time), as the boilerplate URL and the file extension won&#39;t change. From googling30 &quot;nba injury report&quot; and clicking on the second link it seems like the times may be 1:30 PM, 5:30 PM and 8:30 PM. Although from Ed&#39;s example, we see a 01PM time in the URL, not 1:30. So we&#39;ll go ahead and assume the appropriate times URL-wise are 01PM, 05PM and 08PM. Lastly, for simplicity, let&#39;s just focus on the 8:30PM (or 8PM, in URL-speak) file, as it&#39;s likely to be the most complete. We can download the rest later if we want. . Generate a List of URLs to Download . Google helped us sort out what we might use for the time portion of the URL, but the date part is actually a little more complicated than we might like, as we need to know the specific URL of each injury report we want to download. According to the official NBA annoucement, the dates we&#39;re interested in are actually the day before game days, except on the second days of back-to-backs, in which case presumably the injury report comes out the day of the game. . This all sounds complicated, and if there&#39;s one thing I&#39;ve learned about projects like this it&#39;s that nothing saps enthusiasm like complication and extra effort that doesn&#39;t &quot;feel&quot; like it&#39;s in the service of what we&#39;re trying to accomplish40. . So, given what we know, it seems as if we have two options: . Get a copy of the NBA schedule, and try to download reports for days we know there&#39;s a game | Just start downloading [a range of?] dates and see what we get | There&#39;s nothing wrong with taking a stab at option 1 here, but this project is about injury reports, not the NBA schedule, so we&#39;ll start with option 250. . Building a List of Dates: The Easy Way . Programmatically dealing with dates can be frustrating, and doing so in Python is no exception. There&#39;s a whole rabbit hole we can go down, talking about timedelta objects and &quot;date arithmetic&quot; et cetera, but let&#39;s just cut right to the chase and use Pandas. Pandas certainly has its issues, but this is one case where it&#39;ll make our lives much simpler. . So finally...some code! . import pandas as pd . (Okay, so that wasn&#39;t very satisfying. There&#39;s more code coming soon, I promise!) . We&#39;re going to use the pandas.date_range (docs) function to generate, well, a range of dates. We&#39;ll use these dates as portion of the URL schema detailed above, but in order to do so, we need to know what range to generate. . The day before the start of the NBA season until the end of the NBA season seems like a reasonable starting point. If we needed to do this for some number of seasons, we&#39;d want to automate this, but for now, let&#39;s just copy the dates manually. Perusing the wikipedia page for the 2020-2021 NBA season, it looks like the season started on December 22, 2020, and ended on July 20th. From manual testing, https://ak-static.cms.nba.com/referee/injury/Injury-Report_2021-07-20_08PM.pdf is a valid link, and the next day, unsurprisingly, is not. . Remembering our &quot;the injury reports can come out the day before&quot; note from above, this gives us our range: December 21, 2020 through July 20, 2021. . Looking at the docs for pandas.date_range it there&#39;s a ton of parameters we can use, but start and end look like they might be all we need. Now, for real this time, let&#39;s look at some code! . season_dates = pd.date_range(start=&#39;12-21-2020&#39;, end=&#39;07-20-2021&#39;) # Let&#39;s look at the first and last dates in our list print(f&#39;The 2020-21 NBA season started on {season_dates[0]}, and ended on {season_dates[-1]}&#39;) . The 2020-21 NBA season started on 2020-12-21 00:00:00, and ended on 2021-07-20 00:00:00 . (It may not look like much, but if you have any idea how obnoxious this would be without pandas, you&#39;ll be impressed!) Now that we have our list of dates, we just need to add them to our URL schema. Let&#39;s define variables for the parts of the URL that are static (for now, we may want to download other report_time values later): . As a reminder, the URL&#39;s look like this: . https://ak-static.cms.nba.com/referee/injury/Injury-Report_2021-07-20_08PM.pdf . url_stem = &#39;https://ak-static.cms.nba.com/referee/injury/Injury-Report_&#39; report_time = &#39;08PM&#39; . And let&#39;s build an example URL, using the first date from the season_dates list we created above: . print(f&#39;{url_stem}{season_dates[0]}_{report_time}.pdf&#39;) . https://ak-static.cms.nba.com/referee/injury/Injury-Report_2020-12-21 00:00:00_08PM.pdf . Almost, but not quite! It looks like our season_dates[0] object is some sort of datetime-y type thing. But let&#39;s find out what Let&#39;s find out exactly: . print(f&#39;{season_dates[0]} is of type {type(season_dates[0])}&#39;) . 2020-12-21 00:00:00 is of type &lt;class &#39;pandas._libs.tslibs.timestamps.Timestamp&#39;&gt; . So, it&#39;s not quite a standard Python datetime object (docs). But it sure seems like it might behave like one60. That being the case, let&#39;s just go ahead and try to use the standard method we use to process Python dates into strings: strftime(). . . Tip: I could never remember which one was which between strftime and strptime until I leared that strftime() is &quot;string from time&quot; and strptime() is &quot;string parse time&quot; (That sound you hear is the &quot;The More You Know&quot; star whoosing by) . From our sample URL, it looks like we need things in a YYYY-MM-DD format. Luckily, that&#39;s a relatively simple format string, but if you want to try it out and be sure, this interactive site for Python datetime format strings is super cool. . from datetime import datetime . print(f&quot;{url_stem}{datetime.strftime(season_dates[0], &#39;%Y-%m-%d&#39;)}_{report_time}.pdf&quot;) . https://ak-static.cms.nba.com/referee/injury/Injury-Report_2020-12-21_08PM.pdf . . Note: Add note about from imports and changing to double-quotes in f-strings . Success! . Now, let&#39;s build a list of similarly formatted URLs. . season_urls = list() for day in season_dates: season_urls.append(f&quot;{url_stem}{datetime.strftime(day, &#39;%Y-%m-%d&#39;)}_{report_time}.pdf&quot;) . Or, if a list comprehension if you prefer (which seems reasonable here, since we&#39;re building a list): . season_urls = [f&quot;{url_stem}{datetime.strftime(day, &#39;%Y-%m-%d&#39;)}_{report_time}.pdf&quot; for day in season_dates] . Let&#39;s sanity check the first five URLs: . season_urls[:5] . [&#39;https://ak-static.cms.nba.com/referee/injury/Injury-Report_2020-12-21_08PM.pdf&#39;, &#39;https://ak-static.cms.nba.com/referee/injury/Injury-Report_2020-12-22_08PM.pdf&#39;, &#39;https://ak-static.cms.nba.com/referee/injury/Injury-Report_2020-12-23_08PM.pdf&#39;, &#39;https://ak-static.cms.nba.com/referee/injury/Injury-Report_2020-12-24_08PM.pdf&#39;, &#39;https://ak-static.cms.nba.com/referee/injury/Injury-Report_2020-12-25_08PM.pdf&#39;] . Looks like we&#39;re good to go! . Download the PDFs . For simplicity&#39;s sake, we&#39;ll just use the requests module to download the PDFs sequentially. Later, we can see about using asynchronous requests to greatly speed up our downloads, but for now let&#39;s just do things one at a time. . We could do this lazily, with something simple like: . import requests # Loop through our URL list for url in season_urls: # Download the PDF resp = requests.get(url) # Write to disk; Create a filename from the URL filename = url.split(&#39;/&#39;)[-1] with open(filename, &#39;wb&#39;) as f: f.write(resp.content) . With the code above, what happens if there&#39;s no report for a specific day? Or if there&#39;s a network error, either with the server or our own internet? . In general, it&#39;s a good idea to program a little defensively when accessing network assets, especially in cases like this, where there&#39;s a high likelihood of accessing an &quot;invalid&quot; URL (i.e. one with nothing on the other end to download). It&#39;s easy to imagine the downstream PDF processing — that we haven&#39;t written yet 🙂 — breaking when it tries to open an empty file, for example. We could try to handle the error at that point, but it&#39;s probably better to do it here, and only write valid files70. . We&#39;ll assume that accessing a URL with a valid injury report on the other end will return HTTP status code 200 (&quot;Success!&quot;), and one without a PDF will return something else80. To handle this, we&#39;ll use the requests library&#39;s raise_for_status() method, which can raise an exception when we get a non-success status code. We can then catch that exception, and skip writing the file. . Let&#39;s add that to the code above: . . Tip: You&#8217;ll see some code that explicitly tests for a specific status code code using an if statement, but this is almost always a job for a try/except block. One of the core &quot;Pythonic&quot; tenets is &quot;Ask for forgiveness, not permission&quot;. . import requests # Loop through our URL lists for url in season_urls: resp = requests.get(url) try: resp.raise_for_status() # This is the exception to catch for non-200 response codes; this won&#39;t necessarily catch network errors except requests.HTTPError: print(f&#39;No valid PDF at {url}, removing from list&#39;) season_dates.remove(url) # The &quot;else&quot; block of a try/except happens if there isn&#39;t an exception else: filename = url.split(&#39;/&#39;)[-1] # Write to disk print(f&#39;Writing PDF for {url}&#39;) with open(filename, &#39;wb&#39;) as f: f.write(resp.content) . . Important: Note that the .raise_for_status() call is the one that raises the exception, not the .get() call! Make sure raise_for_status() is inside your try/except block! . One thing we might note is that if we want to download the other injury report times, we&#39;ll need to create a new list of URLs. That being the case, perhaps it might make sense to construct the URL right before we use it, instead of all at once before hand. And while we&#39;re at it, perhaps we should make this a function, so we don&#39;t have to download all the files in a loop if we don&#39;t want to. . The function should take two parameters, the date to download, and the time of the report (unsurprisingly, these are the two &quot;dynamic&quot; parts of our URL schema.) We can use the chunks of code from building our URL list above to construct the URL right before we use it. . import requests def download_injury_report(report_date, report_time=&#39;08PM&#39;): # So we can alert an interactive user if something goes wrong from warnings import warn url_stem = &#39;https://ak-static.cms.nba.com/referee/injury/Injury-Report_&#39; # Build the URL url = f&quot;{url_stem}{datetime.strftime(report_date, &#39;%Y-%m-%d&#39;)}_{report_time}.pdf&quot; # Fetch the PDF resp = requests.get(url) try: resp.raise_for_status() # This is the exception to catch for non-200 response codes; this won&#39;t necessarily catch network errors except requests.HTTPError: # Just beccause the caller will have to make sure to handle the &quot;None&quot; case doesn&#39;t mean we can&#39;t alert them # stacklevel=2 cribbed from: https://stackoverflow.com/questions/60083173/warnings-module-prints-part-of-warning-twice warn(f&#39;Could not download URL {url}&#39;, stacklevel=2) return None # The &quot;else&quot; block of a try/except happens if there isn&#39;t an exception else: # Return the content to the caller; they can write it to disk or process it return resp . . Note: You may be surprised that I removed the part that writes the PDF to disk. I&#8217;ll spare you the bloviating, but in general I try to adopt a somewhat &quot;functional&quot; programming style whenever possible. Basically, this means I try not to have lots of things happening inside my functions that someone using them might not be aware of (in this context, these things are called &quot;side effects&quot;). A function called download_injury_report should basically just do that (and nothing else). If we want to save the report we can do that too, but we should be explicit about where and how that is happening. By returning the response object, we can let the caller — us, but us outside this function! — decide what to do with it. . Let&#39;s give our function a shot! We&#39;ll use one of the dates we generated previously (note that we don&#39;t have to provide a time, since we put a default value for the function above). . season_dates[-3] . Timestamp(&#39;2021-07-18 00:00:00&#39;, freq=&#39;D&#39;) . resp = download_injury_report(season_dates[-3]) . &lt;ipython-input-24-22c96f8a63d8&gt;:1: UserWarning: Could not download URL https://ak-static.cms.nba.com/referee/injury/Injury-Report_2021-07-18_08PM.pdf resp = download_injury_report(season_dates[-3]) . Okay, I cheated a bit, and chose a date with no game. But it looks like our warning is working correctly! Let&#39;s download a valid PDF and write it to disk: . resp = download_injury_report(season_dates[0]) # Never can be too careful! if resp: # Rather than convert season_dates[0] to a string, let&#39;s just use the .url attribute # from the requests.response object, and process the filename like we did above filename = resp.url.split(&#39;/&#39;)[-1] with open(filename, &#39;wb&#39;) as f: f.write(resp.content) print(f&#39;Wrote {filename} succesfully&#39;) . Wrote Injury-Report_2020-12-21_08PM.pdf succesfully . If you&#39;ve cloned this notebook and are running it, you should have a copy of Injury-Report_2020-12-21_08PM.pdf in the _notebooks directory of the repo. . Now let&#39;s download the season&#39;s worth, but putting something like the above back in a loop. We&#39;ll make two small modifications though, using two of the best and most indispensable libraries in the modern Python ecosystem. First, we&#39;ll define a data directory to write our files (using the sublime pathlib), so we don&#39;t clutter our _notebooks directory, and secondly, we&#39;ll use the amazing tqdm to monitor our progress, instead of printing a status message90. . . Tip: If you don&#8217;t have tqdm installed (for shame!), conda install -c conda-forge tqdm (or pip install tqdm if you must) will do the trick. You are using a Conda environment for this, right?!? . from tqdm.notebook import tqdm from pathlib import Path # Check out https://stackoverflow.com/questions/50110800/python-pathlib-make-directories-if-they-don-t-exist # for more on the .mkdir() method DATA_DIR = Path.cwd() /&#39;injury_reports&#39; DATA_DIR.mkdir(parents=True, exist_ok=True) for day in tqdm(season_dates): resp = download_injury_report(day) # Never can be too careful! if resp: # Rather than convert season_dates[0] to a string, let&#39;s just use the .url attribute # from the requests.response object, and process the filename like we did above filename = resp.url.split(&#39;/&#39;)[-1] with open(DATA_DIR / filename, &#39;wb&#39;) as f: f.write(resp.content) . &lt;ipython-input-95-ce591fe8e6d1&gt;:10: UserWarning: Could not download URL https://ak-static.cms.nba.com/referee/injury/Injury-Report_2021-03-05_08PM.pdf resp = download_injury_report(day) &lt;ipython-input-95-ce591fe8e6d1&gt;:10: UserWarning: Could not download URL https://ak-static.cms.nba.com/referee/injury/Injury-Report_2021-03-08_08PM.pdf resp = download_injury_report(day) &lt;ipython-input-95-ce591fe8e6d1&gt;:10: UserWarning: Could not download URL https://ak-static.cms.nba.com/referee/injury/Injury-Report_2021-07-04_08PM.pdf resp = download_injury_report(day) &lt;ipython-input-95-ce591fe8e6d1&gt;:10: UserWarning: Could not download URL https://ak-static.cms.nba.com/referee/injury/Injury-Report_2021-07-09_08PM.pdf resp = download_injury_report(day) &lt;ipython-input-95-ce591fe8e6d1&gt;:10: UserWarning: Could not download URL https://ak-static.cms.nba.com/referee/injury/Injury-Report_2021-07-12_08PM.pdf resp = download_injury_report(day) &lt;ipython-input-95-ce591fe8e6d1&gt;:10: UserWarning: Could not download URL https://ak-static.cms.nba.com/referee/injury/Injury-Report_2021-07-15_08PM.pdf resp = download_injury_report(day) &lt;ipython-input-95-ce591fe8e6d1&gt;:10: UserWarning: Could not download URL https://ak-static.cms.nba.com/referee/injury/Injury-Report_2021-07-18_08PM.pdf resp = download_injury_report(day) . Task 2: Extracting Tables from PDF Files . Now that we have our 205 injury report PDFs (culled from 212 days!), it&#39;s time to extract the data in the tables. While I&#39;ve briefly played around with optical character recognition (OCR) tools like Tesseract, strict OCR seemed like a bad fit. Other than that, I wasn&#39;t sure what tool to use, which means...google to the rescue! . At first, I [foolishly] googled for &quot;python parse PDF&quot;, and eventually &quot;python parse PDF text&quot;, thinking it&#39;d be obvious I meant &quot;parse PDF tables&quot;. Apparently clairvoyance isn&#39;t quite built into google [yet], so I ended up checking out packages such as PyPDF2, pdfreader, and eventually pdfminer.six. These tools seem much more oriented toward programmatically creating and manipulating PDF documents, than extracting text. Eventually down the list of google results I came upon this stackoverflow post. One of the answers mentioned tabula and converting PDF tables to dataframes. This seemed promising, . import tabula import pandas as pd . pd.set_option(&#39;display.max_rows&#39;, 500) . injury_reports = sorted(list(DATA_DIR.glob(&#39;*.pdf&#39;))) . injury_reports[0] . PosixPath(&#39;/Users/abramson/git/jeremyabramson/blog/_notebooks/injury_reports/Injury-Report_2020-12-21_08PM.pdf&#39;) . tabula.read_pdf(DATA_DIR / injury_reports[0], pages=&#39;all&#39;) . Game Date Game Time Matchup Team Player Name Current Status Reason . 0 12/22/2020 | 07:00 (ET) | GSW@BKN | Brooklyn Nets | Claxton, Nicolas | Out | Injury/Illness - Right Knee; Tendinopathy | . 1 NaN | NaN | Golden State Warriors | Green, Draymond | Out | Injury/Illness - Right foot; soreness | NaN | . 2 NaN | NaN | NaN | Smailagic, Alen | Out | Injury/Illness - Right Knee; Soreness | NaN | . 3 10:00 (ET) | LAC@LAL | LA Clippers | Morris Sr., Marcus | Out | Injury/Illness - Right Knee; Soreness | NaN | . tabula.read_pdf(DATA_DIR / injury_reports[-1], pages=&#39;all&#39;) . Game Date Game Time Matchup Team Player Name Current Status Reason . 0 07/20/2021 | 09:00 (ET) | PHX@MIL | Milwaukee Bucks | Antetokounmpo, Thanasis | Out | Health and Safety Protocols | . 1 NaN | DiVincenzo, Donte | Out | Injury/Illness - Left Ankle; Surgery | NaN | NaN | NaN | . 2 Phoenix Suns | Saric, Dario | Out | Injury/Illness - Right Acl; Tear | NaN | NaN | NaN | . tabula.read_pdf(DATA_DIR / injury_reports[100], pages=&#39;all&#39;) . df = tabula.read_pdf(injury_reports[40], stream=True, pages=&#39;all&#39;) . df = df.drop(df.loc[df[&#39;Game Date&#39;] == &#39;Game Date&#39;].index).reset_index(drop=True) df[&#39;Game Date&#39;] = df[&#39;Game Date&#39;].fillna(method=&#39;pad&#39;) df[&#39;Matchup&#39;] = df[&#39;Matchup&#39;].fillna(method=&#39;pad&#39;) df[&#39;Team&#39;] = df[&#39;Team&#39;].fillna(method=&#39;pad&#39;) df[&#39;Game Time&#39;] = df[&#39;Game Time&#39;].fillna(method=&#39;pad&#39;) #df[[&#39;Last Name&#39;, &#39;First Name&#39;]] = df[&#39;Player Name&#39;].str.split(&#39;,&#39;, expand=True) #df = df.drop(columns=[&#39;Player Name&#39;]) . df . def clean_injury_report(df): df = df.drop(df.loc[df[&#39;Game Date&#39;] == &#39;Game Date&#39;].index).reset_index(drop=True) df[&#39;Game Date&#39;] = df[&#39;Game Date&#39;].fillna(method=&#39;pad&#39;) df[&#39;Matchup&#39;] = df[&#39;Matchup&#39;].fillna(method=&#39;pad&#39;) df[&#39;Team&#39;] = df[&#39;Team&#39;].fillna(method=&#39;pad&#39;) df[&#39;Game Time&#39;] = df[&#39;Game Time&#39;].fillna(method=&#39;pad&#39;) return df . from pathlib import Path from tqdm.notebook import tqdm . pdfs = sorted(list(Path(&#39;./injury_reports&#39;).glob(&#39;*.pdf&#39;))) . tabula.read_pdf(&#39;2021-05-20_08PM.pdf&#39;, stream=True, guess=False, pages=&#39;all&#39;) . Unnamed: 0 Unnamed: 1 Unnamed: 2 Unnamed: 3 Injury Report: 05/20/21 08:30 PM Unnamed: 5 . 0 Game Date | Game Time | Matchup | Team | Player Name Current Status | Reason | . 1 05/20/2021 | 08:00 (ET) | IND@WAS | Indiana Pacers | Brogdon, Malcolm Available | Injury/Illness - Right Hamstring; Sore | . 2 NaN | NaN | NaN | NaN | Lamb, Jeremy Out | Injury/Illness - Left Knee; Sore | . 3 NaN | NaN | NaN | NaN | LeVert, Caris Out | Health and Safety Protocols | . 4 NaN | NaN | NaN | NaN | Sumner, Edmond Available | Injury/Illness - Left Knee; Contusion | . 5 NaN | NaN | NaN | NaN | Turner, Myles Out | Injury/Illness - Right Toe; Partial Plantar Pl... | . 6 NaN | NaN | NaN | NaN | Warren, T.J. Out | Injury/Illness - Left Foot; Stress Fracture | . 7 NaN | NaN | NaN | Washington Wizards | Avdija, Deni Out | Injury/Illness - Right Ankle; Right ankle frac... | . 8 NaN | NaN | NaN | NaN | Bryant, Thomas Out | Injury/Illness - Left Knee; Left ACL injury | . 9 05/21/2021 | 09:00 (ET) | MEM@GSW | Golden State Warriors | NaN | NOT YET SUBMITTED | . 10 NaN | NaN | NaN | Memphis Grizzlies | McDermott, Sean Out | Injury/Illness - Left Foot; Soreness | . 11 NaN | NaN | NaN | NaN | NaN | NaN | . 12 NaN | NaN | NaN | NaN | Page 1 of 1 | NaN | . tabula.read_pdf(&#39;2021-05-20_08PM.pdf&#39;, stream=True, area=[52.099,16.84,562.561,834.632], pages=&#39;all&#39;) . Game Date Game Time Matchup Team Player Name Current Status Reason . 0 05/20/2021 | 08:00 (ET) | IND@WAS | Indiana Pacers | Brogdon, Malcolm | Available | Injury/Illness - Right Hamstring; Sore | . 1 NaN | NaN | NaN | NaN | Lamb, Jeremy | Out | Injury/Illness - Left Knee; Sore | . 2 NaN | NaN | NaN | NaN | LeVert, Caris | Out | Health and Safety Protocols | . 3 NaN | NaN | NaN | NaN | Sumner, Edmond | Available | Injury/Illness - Left Knee; Contusion | . 4 NaN | NaN | NaN | NaN | Turner, Myles | Out | Injury/Illness - Right Toe; Partial Plantar Pl... | . 5 NaN | NaN | NaN | NaN | Warren, T.J. | Out | Injury/Illness - Left Foot; Stress Fracture | . 6 NaN | NaN | NaN | Washington Wizards | Avdija, Deni | Out | Injury/Illness - Right Ankle; Right ankle frac... | . 7 NaN | NaN | NaN | NaN | Bryant, Thomas | Out | Injury/Illness - Left Knee; Left ACL injury | . 8 05/21/2021 | 09:00 (ET) | MEM@GSW | Golden State Warriors | NaN | NaN | NOT YET SUBMITTED | . 9 NaN | NaN | NaN | Memphis Grizzlies | McDermott, Sean | Out | Injury/Illness - Left Foot; Soreness | . tabula.read_pdf(&#39;2021-05-16_08PM.pdf&#39;, stream=True, pages=&#39;all&#39;) . dfs = list() for pdf in tqdm(pdfs): print(f&#39;processing {pdf}&#39;) df = tabula.read_pdf(pdf, stream=True, area=[52.099,16.84,562.561,834.632], pages=&#39;all&#39;) df = clean_injury_report(df) dfs.append(df) . df = pd.concat(dfs) . df . Game Date Game Time Matchup Team Player Name Current Status Reason . 0 12/22/2020 | 07:00 (ET) | GSW@BKN | Brooklyn Nets | Claxton, Nicolas | Out | Injury/Illness - Right Knee; Tendinopathy | . 1 12/22/2020 | 07:00 (ET) | GSW@BKN | Golden State Warriors | Green, Draymond | Out | Injury/Illness - Right foot; soreness | . 2 12/22/2020 | 07:00 (ET) | GSW@BKN | Golden State Warriors | Smailagic, Alen | Out | Injury/Illness - Right Knee; Soreness | . 3 12/22/2020 | 10:00 (ET) | LAC@LAL | LA Clippers | Morris Sr., Marcus | Out | Injury/Illness - Right Knee; Soreness | . 0 12/22/2020 | 07:00 (ET) | GSW@BKN | Brooklyn Nets | Claxton, Nicolas | Out | Injury/Illness - Right Knee; Tendinopathy | . ... ... | ... | ... | ... | ... | ... | ... | . 1 07/20/2021 | 09:00 (ET) | PHX@MIL | Milwaukee Bucks | DiVincenzo, Donte | Out | Injury/Illness - Left Ankle; Surgery | . 2 07/20/2021 | 09:00 (ET) | PHX@MIL | Phoenix Suns | Saric, Dario | Out | Injury/Illness - Right Acl; Tear | . 0 07/20/2021 | 09:00 (ET) | PHX@MIL | Milwaukee Bucks | Antetokounmpo, Thanasis | Out | Health and Safety Protocols | . 1 07/20/2021 | 09:00 (ET) | PHX@MIL | Milwaukee Bucks | DiVincenzo, Donte | Out | Injury/Illness - Left Ankle; Surgery | . 2 07/20/2021 | 09:00 (ET) | PHX@MIL | Phoenix Suns | Saric, Dario | Out | Injury/Illness - Right Acl; Tear | . 16401 rows × 7 columns . df[&#39;Game Date&#39;].nunique() . 192 . len(season_dates) . 212 . df[&#39;Team&#39;].unique() . array([&#39;Brooklyn Nets&#39;, &#39;Golden State Warriors&#39;, &#39;LA Clippers&#39;, &#39;Charlotte Hornets&#39;, &#39;Cleveland Cavaliers&#39;, &#39;Miami Heat&#39;, &#39;Orlando Magic&#39;, &#39;Indiana Pacers&#39;, &#39;New York Knicks&#39;, &#39;Washington Wizards&#39;, &#39;Boston Celtics&#39;, &#39;Milwaukee Bucks&#39;, &#39;New Orleans Pelicans&#39;, &#39;Toronto Raptors&#39;, &#39;Atlanta Hawks&#39;, &#39;Chicago Bulls&#39;, &#39;Detroit Pistons&#39;, &#39;Minnesota Timberwolves&#39;, &#39;Houston Rockets&#39;, &#39;Oklahoma City Thunder&#39;, &#39;Memphis Grizzlies&#39;, &#39;San Antonio Spurs&#39;, &#39;Denver Nuggets&#39;, &#39;Sacramento Kings&#39;, &#39;Portland Trail Blazers&#39;, &#39;Utah Jazz&#39;, &#39;Dallas Mavericks&#39;, &#39;Phoenix Suns&#39;, &#39;Los Angeles Lakers&#39;, &#39;Philadelphia 76ers&#39;, &#39;Non-NBA Team&#39;], dtype=object) . df[df[&#39;Team&#39;] == &#39;Non-NBA Team&#39;] . Game Date Game Time Matchup Team Player Name Current Status Reason . 0 03/07/2021 | 08:00 (ET) | UNK@UNK | Non-NBA Team | NaN | NaN | NOT YET SUBMITTED | . 1 03/07/2021 | 08:00 (ET) | UNK@UNK | Non-NBA Team | NaN | NaN | NOT YET SUBMITTED | . 0 03/07/2021 | 08:00 (ET) | UNK@UNK | Non-NBA Team | NaN | NaN | NOT YET SUBMITTED | . 1 03/07/2021 | 08:00 (ET) | UNK@UNK | Non-NBA Team | NaN | NaN | NOT YET SUBMITTED | . 10. Famous last &lt;a href=↩ . 20. If you happen to find a central place for these, let me &lt;a href=↩ . 30. I&#39;m going to try my best to show the provenance of as much insight as I can here. I had no idea when the injury reports came out, so I googled it. Don&#39;t be ashamed to do likewise if there&#39;s something you need to know!↩ . 40. &#39;Decoding the NBA schedule&#39; would certainly be something we could have added to our outline. This just shows that something as seemingly simple as &#39;download the PDFs&#39; requires a number of steps that might not be readily apparent!↩ . 50. ...and if we happen to run into Adam Silver at a Christmas party, be sure to apologize for the extra traffic on the NBA.com servers↩ . 60. The notion that &#39;something is (e.g.) a list because it behaves like a list&#39; is basically the meaning behind &lt;a href=↩ . 70. If you&#39;re thinking &#39;You should do both&#39;, you get a gold star!↩ . 80. From manual inspection of the HTTP headers, it looks like nba.com returns a 403 (&#39;Forbidden&#39;) when there isn&#39;t a valid PDF↩ . 90. Or, we could explore the wonderful world of &lt;a href=↩ . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} .",
            "url": "https://jeremyabramson.github.io/blog/sports%20analytics/nba/python/research/projects/injuries/2021/07/25/nba-injuries.html",
            "relUrl": "/sports%20analytics/nba/python/research/projects/injuries/2021/07/25/nba-injuries.html",
            "date": " • Jul 25, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Datetime vs. Timedeltas",
            "content": "import datetime random_date = datetime.datetime.strptime(&#39;2011-09-18&#39;, &#39;%Y-%m-%d&#39;) another_date = datetime.datetime.strptime(&#39;2021-01-1&#39;, &#39;%Y-%m-%d&#39;) ten_days_hence = datetime.timedelta(days=10) # datetime - datetime = timedelta print(&#39;******: datetime - datetime = timedelta&#39;) print(f&#39;A {type(random_date)} - a {type(another_date)} is a {type(random_date - another_date)}&#39;) print(f&#39;i.e. {random_date} - {another_date} = {random_date - another_date}&#39;) # datetime + datetime = Error! print(&#39;******: datetime + datetime = Error!&#39;) try: print(f&#39;A {type(random_date)} + a {type(another_date)} is a {type(random_date + another_date)}&#39;) except TypeError: print(f&quot;You can&#39;t add {type(random_date)=} and {type(another_date)=}&quot;) # datetime +/- timedelta = datetime print(&#39;******: datetime +/- timedelta = datetime&#39;) print(f&#39;A {type(random_date)} +/- a {type(ten_days_hence)} is a {type(random_date - ten_days_hence)}&#39;) print(f&#39;i.e. {random_date} - {ten_days_hence} = {random_date - ten_days_hence}&#39;) print(f&#39;i.e. {random_date} + {ten_days_hence} = {random_date + ten_days_hence}&#39;) # timedelta + datetime = datetime print(&#39;******: timedelta + datetime = datetime&#39;) print(f&#39;A {type(ten_days_hence)} + a {type(random_date)} is a {type(ten_days_hence + random_date)}&#39;) print(f&#39;i.e. {ten_days_hence} + {random_date} = {ten_days_hence + random_date}&#39;) # timedelta - datetime = Error! print(&#39;******: # timedelta - datetime = Error!&#39;) try: print(f&#39;A {type(ten_days_hence)} - a {type(random_date)} is a {type(ten_days_hence - random_date)}&#39;) except TypeError: print(f&quot;You can&#39;t subtract {type(ten_days_hence)=} - {type(random_date)=}&quot;) new_date = random_date + datetime.timedelta(days=10) # timedelta = datetime +/- datetime other_date = new_date - random_date # datetime = timedelta + datetime yet_another_date = datetime.timedelta(days=-10) + random_date # ERROR = timedelta - datetime # bad_value = datetime.timedelta(days=-10) - random_date . ******: datetime - datetime = timedelta A &lt;class &#39;datetime.datetime&#39;&gt; - a &lt;class &#39;datetime.datetime&#39;&gt; is a &lt;class &#39;datetime.timedelta&#39;&gt; i.e. 2011-09-18 00:00:00 - 2021-01-01 00:00:00 = -3393 days, 0:00:00 ******: datetime + datetime = Error! You can&#39;t add type(random_date)=&lt;class &#39;datetime.datetime&#39;&gt; and type(another_date)=&lt;class &#39;datetime.datetime&#39;&gt; ******: datetime +/- timedelta = datetime A &lt;class &#39;datetime.datetime&#39;&gt; +/- a &lt;class &#39;datetime.timedelta&#39;&gt; is a &lt;class &#39;datetime.datetime&#39;&gt; i.e. 2011-09-18 00:00:00 - 10 days, 0:00:00 = 2011-09-08 00:00:00 i.e. 2011-09-18 00:00:00 + 10 days, 0:00:00 = 2011-09-28 00:00:00 ******: timedelta + datetime = datetime A &lt;class &#39;datetime.timedelta&#39;&gt; + a &lt;class &#39;datetime.datetime&#39;&gt; is a &lt;class &#39;datetime.datetime&#39;&gt; i.e. 10 days, 0:00:00 + 2011-09-18 00:00:00 = 2011-09-28 00:00:00 ******: # timedelta - datetime = Error! You can&#39;t subtract type(ten_days_hence)=&lt;class &#39;datetime.timedelta&#39;&gt; - type(random_date)=&lt;class &#39;datetime.datetime&#39;&gt; . . Tip: If you were curious, this can be converted to a generator with the following: . def daterange(date1, date2): for n in range(int ((date2 - date1).days)+1): yield date1 + datetime.timedelta(n) . def daterange(date1, date2): for n in range(int ((date2 - date1).days)+1): yield date1 + datetime.timedelta(n) . for thing in daterange(season_start, season_end): print(thing) .",
            "url": "https://jeremyabramson.github.io/blog/sports%20analytics/nba/python/research/projects/injuries/2021/07/21/datetime-vs-timedelta.html",
            "relUrl": "/sports%20analytics/nba/python/research/projects/injuries/2021/07/21/datetime-vs-timedelta.html",
            "date": " • Jul 21, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://jeremyabramson.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://jeremyabramson.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://jeremyabramson.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jeremyabramson.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}